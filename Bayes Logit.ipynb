{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Card Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a common benchmark model in machine learning because of its simplicity (no parameter tuning in a model without regularization), robustness (not prune to overfitting) and well-understood theoretical background. It's also a common model in many fields for researchers looking to analyze data to empirically test their theories, because its coefficients are easy to interpret. Logistic regression is also a good starting point to understand how Bayesian modeling works and to check out Edward (http://edwardlib.org), a new framework build on tensorflow to make Baysian methods faster and easier to use in combination with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll work through an experiment to understand the necessary steps and see if I can get the code right. The data comes from a marketing experiment/conjoint analysis where participants were shown a choice between two hypothetical bank cards. The two cards were identical in all characteristics except for two.  Each participant evaluated 13-17 paired comparisons \n",
    "involving a fraction of attributes. \n",
    "\n",
    "The data bank_choice contains columns:\n",
    "\n",
    "- id: participant identifier\n",
    "- choice: choice indicator (1=first card; 0=second card)\n",
    "- remaining columns: x_hi1 - x_hi2, difference in attribute x between card 1 and card 2\n",
    "\n",
    "The data bank_demo contains columns:\n",
    "\n",
    "- id: participant identifier\n",
    "- remaining columns: demographic information\n",
    "\n",
    "Data Source:\n",
    "Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2003). Bayesian Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # fast standard math capabilities\n",
    "import pandas as pd # easy data handling\n",
    "from sklearn.model_selection import train_test_split # scikit-learn for ML\n",
    "import edward as ed \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt # Useful for plotting\n",
    "import seaborn as sns # Advanced plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv data using pandas\n",
    "bank_choice = pd.read_csv(\"/Users/hauptjoh/Downloads/bank_choice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'choice', 'Med_FInt', 'Low_FInt', 'Med_VInt', 'Rewrd_2',\n",
      "       'Rewrd_3', 'Rewrd_4', 'Med_Fee', 'Low_Fee', 'Bank_B', 'Out_State',\n",
      "       'Med_Rebate', 'High_Rebate', 'High_CredLine', 'Long_Grace'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check column names\n",
    "print(bank_choice.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, I split the data into a subset to train the model (70%) and a different subset (30%) to check its performance. I also separate the target variable *choice* and the model input at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_train, choice_test = train_test_split(bank_choice, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = choice_train[[\"choice\"]].values.flatten()\n",
    "X_train = choice_train.drop([\"choice\", \"id\"], axis = 1).values\n",
    "\n",
    "y_test = choice_test[[\"choice\"]].values.flatten()\n",
    "X_test = choice_test.drop([\"choice\", \"id\"], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10359,)\n",
      "(10359, 14)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (frequentist) Logit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you search for 'logistic regression with python', there's tons of pages explaining how to estimate a logistic regression model using sklearn. For comparison, I'll get the coefficients and area-under-the-ROC-curve (AUC) from the standard logit implementation.\n",
    "\n",
    "To be specific, I train the model $P(y_{hi}=1)=\\frac{exp[(x_{hi1}-x_{hi2})^T\\beta]}{1+exp[(x_{hi1}-x_{hi2})^T\\beta]}$,\n",
    "\n",
    "where $\\beta$ are the coefficients to estimate and $x_{hi1}$ and $x_{hi2}$ are the characteristics of choice 1 and choice 2 for the $i$-th set of choices for person $h$.\n",
    "\n",
    "We estimate the coefficients by maximizing the log-likelihood. An important extension to the logit model is regularization by the L1- (LASSO) or L2-norm (ridge). Intuitively, we add a new penalty term to the target function that decreases performance with increasing size of coefficients to make the model more \"simple\". Without Bayesian reasoning, this has some clear practical advantages, but to my knowledge no convincing theory behind it beyond \"when we force the coefficients to be smaller, the model is less complex and less complex models are more general/less prone to overfit\".\n",
    "\n",
    "Is not having a theoretical reason a bad thing? I know from neural networks that many practical tricks seem to work very robustly without a general proof. But not completely understanding an approach leaves us in the dark about its details, e.g. what it means to choose the L1-, L2- or why not the L4-norm? It blew my mind to find out that there's a clear Bayesian intuition for the L2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.59500819  3.10176761  2.01029561 -0.05008957 -0.47475756 -0.59287675\n",
      "   1.27482891  2.43007257 -0.09189518 -1.33641466  0.8876274   1.51297273\n",
      "   0.58670077  1.90118752]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with scikit-learn\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LogisticRegression(penalty=\"l2\", C=1)\n",
    "reg.fit(X=X_train, y=y_train)\n",
    "# Print out the estimated coefficients\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7924568382159967\n"
     ]
    }
   ],
   "source": [
    "# ROC-AUC for logit model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test, reg.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Logit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do logistic regression using Bayesian methods implemented in *Edward*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with edward, it's useful to have an intuition about how Tensorflow works. Tensorflow creates a dataflow graph in the background. If you've worked with module *Keras* before to build neural networks, you'll notice that Edward requires a little more interaction with actual tensorflow. Most importantly, remember that tensorflow will create a session in the background where all the Tensorflow variables and operations are defined. For future reference and as a reminder to myself, here's the link to Tensorflow's help page on this: https://www.tensorflow.org/programmers_guide/graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If things go wrong, it's useful to reset the graph and start fresh\n",
    "# without having to restart the kernel\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number generator seed for replicable results\n",
    "ed.set_seed(42) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we are constructing a computational graph in the background. Without understanding any details, that means that we need to be specific about the input to the computation. The way this works is that we define a placeholder with the right dimensions into which we can input data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10359\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "N = y_train.shape[0] # Number of observations\n",
    "D = X_train.shape[1] # Number of variables\n",
    "print(N)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow placeholder for matrix of float numbers with dimensions [N, D]\n",
    "X = tf.placeholder(tf.float32, [N, D])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll define the model or \"generative process\" of the data. It's useful to start at the end of the definition. \n",
    "\n",
    "- $y$: The observations $y$ are sampled from a Bernoulli distribution because that distribution describes the \"success\" of an experiment in terms of 0 and 1. It takes one parameter, which is the probability of the intresting outcome,i.e. person picks credit card option 1. We believe that this probability is different for each set of options, so we make the probability dependent on $x$.\n",
    "\n",
    "At this point, we could maximize the likelihood of the observed outcomes with regard to the model parameters. Maximizing the likelihood sounds complicated, but the steps are not actually difficult (in this case). *Bernoulli distribution* is just shorthand for a function that we can look up on Wikipedia, \n",
    "\n",
    "$f(y,p)=p^{k}(1-p)^{1-y} \\quad \\text{for } y \\in \\{0,1\\}$. \n",
    "\n",
    "Since our observations are (assumed to be) independent, the overall likelihood is the product of the likelihood of each observation.\n",
    "\n",
    "This is the standard approach. Why is that not the best we can do? I'll try to paraphrase Christopher Bishop's great book Pattern Recognition and Machine Learning. Our model captures one kind of uncertainty: We take what we know about the credit card options and say how probable it is that a person will choose option 1.    \n",
    "The standard model ignores another kind of uncertainty: We are not sure about our estimate of the coefficients, they could be a little higher or lower. We could be more certain about our estimates if, for example, we had more data, but some uncertainty will remain.    \n",
    "Typically, I would address these concerns by looking at the size and p-values of the estimated effects and so on. But in the Bayesian framework, we can include them into the model to estimate the uncertainty directly.\n",
    "\n",
    "- $b$ & $w$: I draw the bias term and each coefficient/weight of the regression part from a Normal distribution with mean 0 and variance equal to 1. But... why? I believe that the effects are likely to lie around 0 (at least since the data is normalized) and less likely to lie far away from zero. That's my *prior* belief and a design choice. The interesting part is that with enough data, the prior beliefs matter less and less and may be completely overruled.\n",
    "\n",
    "If you've paid attention, you'll notice that I have not said how I picked the mean and variance for the normal distributions. I could also include a prior on those or estimate them from the data. Instead, I will argue that it's okay to just pick them in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.models import Normal, Bernoulli, Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Normal(loc=tf.zeros(D), scale=1.0 * tf.ones(D))\n",
    "b = Normal(loc=tf.zeros([]), scale=1.0 * tf.ones([]))\n",
    "y = Bernoulli(probs=1/(1+tf.exp(-(ed.dot(X, w) + b))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli class can take the log-odds (logit) as argument directly, thus avoiding the calculation of the expectation, but I feel that it's helpful to make the code look like the definition of the Bernoulli distirbution that you'll find online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomVariable(\"Bernoulli/\", shape=(10359,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still write down the function behind the model including the priors, but it is not as pretty as before:\n",
    "\n",
    "$p(y|w,b,X) = \\displaystyle\\prod^n_{i=1}\\left[\n",
    "\\left(\\frac{\n",
    "e^{\\beta_0+\\beta_1X_{i1} + \\ldots + \\beta_1X_{i1}}}{1+e^{\\beta_0+\\beta_1X_{i1} + \\ldots + \\beta_1X_{i1}}}\\right)^{y_i}\n",
    "\\left(1-\\frac{\n",
    "e^{\\beta_0+\\beta_1X_{i1} + \\ldots + \\beta_1X_{i1}}}{1+e^{\\beta_0+\\beta_1X_{i1} + \\ldots + \\beta_1X_{i1}}}\\right)^{1-y_i}\\right]\n",
    "\\cdot\n",
    "\\displaystyle\\prod^k_{j=1} \\frac{1}{\\sqrt{2\\pi} \\sigma_j}\\cdot e^{-\n",
    " \\frac{(\\beta_j-\\mu_j)^2}{2\\sigma_j^2} }$\n",
    " \n",
    "Despite looking complicated, I have just written out the definitions for the Bernoulli and Normal distributions. Dealing with complex posterior distributions like this is the difficulty of Bayesian inference and probably the reason why I didn't learn about Bayesian methods in my (under)graduate classes. But we don't have to worry about this, if we don't want to build our own fancy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sampling algorithms to approximate the distribution we are looking for (For a good overview look here: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pdf). This works even in cases where this posterior distribution is much more complicated. \n",
    "The Toyota of sampling alogrithms is the metropolis-hastings algorithm. The MH algorithm works by starting with the parameters (our $b$ and $w$s) initialized at random. It then starts moving through different values of $b$ and $w$ by picking new candidates $b^*, w^*$ and accepting them if they seem 'better'. 'Better' is defined by an acceptance rule giving the probability of acceptance as \n",
    "\n",
    "$\\alpha = \\text{min}(\\frac{p(x^*) q(x|x^*)}{p(x) q(x^*|x)}, 1)$. \n",
    "\n",
    "Don't worry about the $min(\\cdot,1)$, it just ensures that our probability to jump doesn't exceed 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put that in a familiar context, image you're traveling through Germany. You have no idea were tourists normally go, so you decide to pick you next destination on the fly. Let's say your traveling for good and if you find the perfect place, you are willing to go back for another day. That's a Markov Chain, since your next destination depends only on your current position. \n",
    "\n",
    "Today you are in Frankfurt and searching for the next train to leave the city $q(\\cdot|x)$ suggets you go to Nuremberg. To decide whether to go or stay, you check online 1) how many tourists visit Frankfurt every year $p(x)$ and 2) how many tourists visit Nuremberg every year $p(x^*)$. If relatively more tourists visit Nuremberg, that's surely the place to be, so you're more likely to go. That's the original Metropolis algorithm.\n",
    "\n",
    "Now a more efficient backpacker strategy would be to ask a random passer-by for a suggestion instead of boarding just any train. Sounds good, but there's a danger: They could make you miss out on whole parts of the country by sending you to places where people are unlikely to send you back in the direction you came from. Let's say Germans from the south are unlikely to send you back up north to Hamburg or Berlin. In this case it's better to think twice before burning that bridge and going to Nuremberg, so you correct your acceptance probability to account for the asymmetry. You can do that by taking surprising suggetions $q(x^*|x)$ more seriously and avoiding suggestions that send you to places where people will not send you back the direction you came from $p(x|x^*)$. That's the extension from the Metropolis to the Metropolis-Hastings algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tricky part in application of the MH algorithm is to pick a proposal distribution that gives suggestions far from the places you've already seen but also enough suggestions that fit the acceptance rule. Throwing darts might suggest too many places all over Germany that no tourist has ever visited, so you'll just stay put in Frankfurt. Passers-by are likely to suggest cities in the vicinity and get you stuck in just one state for a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, we'll sample new candiates from a normal distribution centered around the current state. If we pick the variance of the normal distirbution too high, we'll end up with a low acceptance rate and no movement of the markov chain. If we pick the variance too low, the chain will move in small steps and will take too long to explore. I've picked 0.2 by trying a few values and aiming for an acceptance rate close to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_w = Normal(loc=w, scale = .02)\n",
    "proposal_b = Normal(loc=b, scale = .02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to take *n* samples from our model, so we need to create a place to save them. Edward has a class *Empirical* which collects samples from a random variable. It then gives us direct access to the empirical statistics of the random variables and I can sample directly from it. I need to set up space to save the random variables, so I pick the number of samples that I would like to save at 30,000. I pick 30,000 because I sounds high enough to get robust results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30000 # Number of iterations\n",
    "qw = Empirical(tf.Variable(tf.zeros([T,D])))\n",
    "qb = Empirical(tf.Variable(tf.zeros([T,])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now put it all together. I would like to do inference on variables connected through the model I specified using the MH algorithm. The necessary information is\n",
    "\n",
    "- Use function MetropolisHastings to sample\n",
    "- Save the results for w in a random variable qw, for b in a random variable qb\n",
    "- During sampling, samples new proposed steps from the distributions proposal_w and proposal_b\n",
    "- Evaluate the target posterior given data X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hauptjoh/anaconda/envs/edward/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/hauptjoh/anaconda/envs/edward/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    }
   ],
   "source": [
    "from edward.inferences import MetropolisHastings\n",
    "inference = MetropolisHastings({w:qw, b:qb}, \n",
    "                               {w:proposal_w, b:proposal_b},\n",
    "                               data={y:y_train, X:X_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [100%] ██████████████████████████████ Elapsed: 72s | Acceptance Rate: 0.413\n"
     ]
    }
   ],
   "source": [
    "# Actually run inference and print update every 500 iterations\n",
    "inference.run(n_print=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what that gets us. Contracy to the frequentist approach, our result is a distribution for each parameter $b,w_1,\\ldots,w_{14}$. What does that mean? Simply put, we are not sure what the parameters are! That sounds bad, but is actually in the nature of things. \n",
    "\n",
    "The philosopher David Hume made a pretty convincing argument that even if I've seen the sun rise my whole life, I can never be 100% certain that it will do so tomorrow, since logical certainties can't be deducted from observation only. I can be very, very certain, though, and we can visualize that certainty. I'll visualize each distribution by sampling from it a lot of times and plotting the samples as a histogram or density plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For variable at position var_id, sample n times and plot\n",
    "var_id = 0\n",
    "# Sample from the random variable's distribution 10000 times\n",
    "# Don't forget to evaluate the tensors after sampling to get numpy arrays.\n",
    "estimates = qw.sample(10000).eval()[:, var_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a20d60eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute a density plot to have a smooth distribution of results\n",
    "var_plot = sns.kdeplot(data=estimates)\n",
    "# Add lines at density mean (blue) and at sklearn regression estimate (red)\n",
    "plt.axvline(x=np.mean(estimates), color = \"blue\", linestyle=\"dashed\")\n",
    "plt.axvline(x=reg.coef_.flatten()[var_id], color = \"red\", linestyle=\"dashed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this? I am used to seeing three things reported in studies from economics: a single estimate of the effect, a p-value and a confidence interval. How to we find a single effect estimate to report? The mean? The median? One important takeaway is that the standard approach picks a single value for us that might not be the one we would pick ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.059135817\n",
      "[-0.04339837]\n"
     ]
    }
   ],
   "source": [
    "b_est = np.mean(qb.sample(1000).eval())\n",
    "print(b_est)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.5769122   3.0697756   1.9952586  -0.04674401 -0.46992552 -0.588219\n",
      "  1.2472618   2.3820918  -0.09895556 -1.3471849   0.8685688   1.4687728\n",
      "  0.5630965   1.8716323 ]\n",
      "[[ 1.59500819  3.10176761  2.01029561 -0.05008957 -0.47475756 -0.59287675\n",
      "   1.27482891  2.43007257 -0.09189518 -1.33641466  0.8876274   1.51297273\n",
      "   0.58670077  1.90118752]]\n"
     ]
    }
   ],
   "source": [
    "w_est = np.mean(qw.sample(1000).eval(),axis=0)\n",
    "print(w_est)\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With point estimates from the mean of the distribution, we can follow the same math as in frequentist logistic regression to calculate probability estimates. In fact, if you're willing to do some math, you can show that this is equivalent to maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.sigmoid(ed.dot(tf.cast(X_test, tf.float32), w_est) + b_est).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25302917 0.37208635 0.5394462  0.13802248 0.1548903  0.19252193\n",
      " 0.6254152  0.48546207 0.69709945 0.32713148]\n"
     ]
    }
   ],
   "source": [
    "print(pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7922674666927396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "print(roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get true Bayesian estimates, we can avoid point estimates and instead sample from the posterior distribution of y. The resulting samples can be used to calculate better measures of our confidence in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting on the test data, an issue with Edward that I haven't figured out is that the placeholder for the input data doesn't fit anymore. We defined the placeholder to be the size of the training data for inference, but now need it to be the size of the test data. I therefore copy the posterior with the infered weights and bias distributions and a new placeholder for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_post = tf.placeholder(tf.float32, X_test.shape)\n",
    "y_post = Bernoulli(logits=ed.dot(X_post, qw) + qb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample predictions for all observations from the Bernoulli distribution (i.e. 1/0). Repeat drawing predictions n times to approximate a prediction distribution for each observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4440)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = y_post.sample(50000, ).eval({X_post:X_test})\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0:5,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFl9JREFUeJzt3X/MXuV93/H3JzaQbPlhJzyJkG1m1rpaHKQ65Bl4irSlEIEhU0wlUhmtxY2suctgSreoC3R/kCZBCptSJiRC5xQPE7UxjP7Aos48ixBlmcKPh0IAQ5GfAgtPjWKnNjQRKpnZd3/cl6t7Prf93M8PP7d/vF/S0XPO91zn3NeFjT8+51z3caoKSZL6vW3UHZAknXwMB0lSh+EgSeowHCRJHYaDJKnDcJAkdQwdDkkWJXkyyYNt+4IkjybZm+TeJGe3+jlte7LtX9l3jpta/YUkV/TV17XaZJIb5294kqTZmMmVw2eB5/u2bwVuq6pVwCFgU6tvAg5V1c8Dt7V2JFkNbAA+BKwDvtYCZxFwB3AlsBq4trWVJI3IUOGQZDnwCeD323aAS4H7W5NtwNVtfX3bpu2/rLVfD2yvqjer6iVgEri4LZNV9WJV/QzY3tpKkkZk8ZDt/jPw74F3te33Aa9V1eG2PQUsa+vLgFcAqupwktdb+2XAI33n7D/mlaPql0zXoXPPPbdWrlw5ZPclSQBPPPHEj6tqbLp204ZDkn8O7K+qJ5J87Eh5QNOaZt+x6oOuXga+0yPJZmAzwPnnn8/ExMRxei5JOlqS/z1Mu2FuK30U+GSSl+nd8rmU3pXEkiRHwmU5sK+tTwErWicWA+8BDvbXjzrmWPWOqtpSVeNVNT42Nm3wSZJmadpwqKqbqmp5Va2k90D521X1L4CHgWtas43AA219R9um7f929d7utwPY0GYzXQCsAh4DHgdWtdlPZ7fP2DEvo5MkzcqwzxwG+TywPcmXgSeBu1r9LuAbSSbpXTFsAKiqPUnuA54DDgPXV9VbAEluAHYBi4CtVbVnDv2SJM1RTtVXdo+Pj5fPHCRpZpI8UVXj07XzG9KSpA7DQZLUYThIkjoMB0lSh+EgSeqYy1TWU9bKG/9sJJ/78lc+MZLPlaSZ8spBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeqYNhySvD3JY0l+kGRPkt9p9buTvJTkqbasafUkuT3JZJKnk1zUd66NSfa2ZWNf/SNJnmnH3J4kJ2KwkqThDPNW1jeBS6vqp0nOAr6X5Ftt329V1f1Htb8SWNWWS4A7gUuSvBe4GRgHCngiyY6qOtTabAYeAXYC64BvIUkaiWmvHKrnp23zrLbUcQ5ZD9zTjnsEWJLkPOAKYHdVHWyBsBtY1/a9u6q+X1UF3ANcPYcxSZLmaKhnDkkWJXkK2E/vD/hH265b2q2j25Kc02rLgFf6Dp9qtePVpwbUJUkjMlQ4VNVbVbUGWA5cnORC4CbgHwH/GHgv8PnWfNDzgppFvSPJ5iQTSSYOHDgwTNclSbMwo9lKVfUa8B1gXVW92m4dvQn8V+Di1mwKWNF32HJg3zT15QPqgz5/S1WNV9X42NjYTLouSZqBYWYrjSVZ0tbfAXwc+Iv2rIA2s+hq4Nl2yA7gujZraS3welW9CuwCLk+yNMlS4HJgV9v3kyRr27muAx6Y32FKkmZimNlK5wHbkiyiFyb3VdWDSb6dZIzebaGngH/V2u8ErgImgTeATwNU1cEkXwIeb+2+WFUH2/pngLuBd9CbpeRMJUkaoWnDoaqeBj48oH7pMdoXcP0x9m0Ftg6oTwAXTtcXSdLC8BvSkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjqmDYckb0/yWJIfJNmT5Hda/YIkjybZm+TeJGe3+jlte7LtX9l3rpta/YUkV/TV17XaZJIb53+YkqSZGObK4U3g0qr6RWANsC7JWuBW4LaqWgUcAja19puAQ1X188BtrR1JVgMbgA8B64CvJVmUZBFwB3AlsBq4trWVJI3ItOFQPT9tm2e1pYBLgftbfRtwdVtf37Zp+y9LklbfXlVvVtVLwCRwcVsmq+rFqvoZsL21lSSNyFDPHNrf8J8C9gO7gb8EXquqw63JFLCsrS8DXgFo+18H3tdfP+qYY9UH9WNzkokkEwcOHBim65KkWRgqHKrqrapaAyyn9zf9Dw5q1n7mGPtmWh/Ujy1VNV5V42NjY9N3XJI0KzOarVRVrwHfAdYCS5IsbruWA/va+hSwAqDtfw9wsL9+1DHHqkuSRmSY2UpjSZa09XcAHweeBx4GrmnNNgIPtPUdbZu2/9tVVa2+oc1mugBYBTwGPA6sarOfzqb30HrHfAxOkjQ7i6dvwnnAtjar6G3AfVX1YJLngO1Jvgw8CdzV2t8FfCPJJL0rhg0AVbUnyX3Ac8Bh4PqqegsgyQ3ALmARsLWq9szbCCVJMzZtOFTV08CHB9RfpPf84ej63wKfOsa5bgFuGVDfCewcor+SpAXgN6QlSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKlj2nBIsiLJw0meT7InyWdb/QtJ/irJU225qu+Ym5JMJnkhyRV99XWtNpnkxr76BUkeTbI3yb1Jzp7vgUqShjfMlcNh4HNV9UFgLXB9ktVt321VtaYtOwHavg3Ah4B1wNeSLEqyCLgDuBJYDVzbd55b27lWAYeATfM0PknSLEwbDlX1alX9eVv/CfA8sOw4h6wHtlfVm1X1EjAJXNyWyap6sap+BmwH1icJcClwfzt+G3D1bAckSZq7GT1zSLIS+DDwaCvdkOTpJFuTLG21ZcArfYdNtdqx6u8DXquqw0fVB33+5iQTSSYOHDgwk65LkmZg6HBI8k7gj4DfrKq/Ae4Efg5YA7wKfPVI0wGH1yzq3WLVlqoar6rxsbGxYbsuSZqhxcM0SnIWvWD4g6r6Y4Cq+lHf/q8DD7bNKWBF3+HLgX1tfVD9x8CSJIvb1UN/e0nSCAwzWynAXcDzVfW7ffXz+pr9MvBsW98BbEhyTpILgFXAY8DjwKo2M+lseg+td1RVAQ8D17TjNwIPzG1YkqS5GObK4aPArwHPJHmq1X6b3myjNfRuAb0M/AZAVe1Jch/wHL2ZTtdX1VsASW4AdgGLgK1Vtaed7/PA9iRfBp6kF0aSpBGZNhyq6nsMfi6w8zjH3ALcMqC+c9BxVfUivdlMkqSTgN+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkjmnDIcmKJA8neT7JniSfbfX3JtmdZG/7ubTVk+T2JJNJnk5yUd+5Nrb2e5Ns7Kt/JMkz7Zjbkwz6Z0klSQtkmCuHw8DnquqDwFrg+iSrgRuBh6pqFfBQ2wa4EljVls3AndALE+Bm4BJ6/170zUcCpbXZ3HfcurkPTZI0W9OGQ1W9WlV/3tZ/AjwPLAPWA9tas23A1W19PXBP9TwCLElyHnAFsLuqDlbVIWA3sK7te3dVfb+qCrin71ySpBGY0TOHJCuBDwOPAh+oqlehFyDA+1uzZcArfYdNtdrx6lMD6oM+f3OSiSQTBw4cmEnXJUkzMHQ4JHkn8EfAb1bV3xyv6YBazaLeLVZtqarxqhofGxubrsuSpFkaKhySnEUvGP6gqv64lX/UbgnRfu5v9SlgRd/hy4F909SXD6hLkkZkmNlKAe4Cnq+q3+3btQM4MuNoI/BAX/26NmtpLfB6u+20C7g8ydL2IPpyYFfb95Mka9tnXdd3LknSCCweos1HgV8DnknyVKv9NvAV4L4km4AfAp9q+3YCVwGTwBvApwGq6mCSLwGPt3ZfrKqDbf0zwN3AO4BvtUWSNCLThkNVfY/BzwUALhvQvoDrj3GurcDWAfUJ4MLp+iJJWhh+Q1qS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjqmDYckW5PsT/JsX+0LSf4qyVNtuapv301JJpO8kOSKvvq6VptMcmNf/YIkjybZm+TeJGfP5wAlSTM3zJXD3cC6AfXbqmpNW3YCJFkNbAA+1I75WpJFSRYBdwBXAquBa1tbgFvbuVYBh4BNcxmQJGnupg2HqvoucHDI860HtlfVm1X1EjAJXNyWyap6sap+BmwH1icJcClwfzt+G3D1DMcgSZpnc3nmcEOSp9ttp6Wttgx4pa/NVKsdq/4+4LWqOnxUfaAkm5NMJJk4cODAHLouSTqe2YbDncDPAWuAV4GvtnoGtK1Z1Aeqqi1VNV5V42NjYzPrsSRpaItnc1BV/ejIepKvAw+2zSlgRV/T5cC+tj6o/mNgSZLF7eqhv70kaURmdeWQ5Ly+zV8Gjsxk2gFsSHJOkguAVcBjwOPAqjYz6Wx6D613VFUBDwPXtOM3Ag/Mpk+SpPkz7ZVDkm8CHwPOTTIF3Ax8LMkaereAXgZ+A6Cq9iS5D3gOOAxcX1VvtfPcAOwCFgFbq2pP+4jPA9uTfBl4Erhr3kYnSZqVacOhqq4dUD7mH+BVdQtwy4D6TmDngPqL9GYzSZJOEn5DWpLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUsesXtktSWe6lTf+2Ug+9+WvfGJBPscrB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqSOacMhydYk+5M821d7b5LdSfa2n0tbPUluTzKZ5OkkF/Uds7G135tkY1/9I0meacfcniTzPUhJ0swMc+VwN7DuqNqNwENVtQp4qG0DXAmsastm4E7ohQlwM3AJvX8v+uYjgdLabO477ujPkiQtsGnDoaq+Cxw8qrwe2NbWtwFX99XvqZ5HgCVJzgOuAHZX1cGqOgTsBta1fe+uqu9XVQH39J1LkjQis33m8IGqehWg/Xx/qy8DXulrN9Vqx6tPDagPlGRzkokkEwcOHJhl1yVJ05nvB9KDnhfULOoDVdWWqhqvqvGxsbFZdlGSNJ3ZhsOP2i0h2s/9rT4FrOhrtxzYN019+YC6JGmEZhsOO4AjM442Ag/01a9rs5bWAq+32067gMuTLG0Poi8HdrV9P0myts1Suq7vXJKkEZn2raxJvgl8DDg3yRS9WUdfAe5Lsgn4IfCp1nwncBUwCbwBfBqgqg4m+RLweGv3xao68pD7M/RmRL0D+FZbJEkjNG04VNW1x9h12YC2BVx/jPNsBbYOqE8AF07XD0nSwvEb0pKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1DGncEjycpJnkjyVZKLV3ptkd5K97efSVk+S25NMJnk6yUV959nY2u9NsnFuQ5IkzdV8XDn8UlWtqarxtn0j8FBVrQIeatsAVwKr2rIZuBN6YQLcDFwCXAzcfCRQJEmjcSJuK60HtrX1bcDVffV7qucRYEmS84ArgN1VdbCqDgG7gXUnoF+SpCHNNRwK+B9JnkiyudU+UFWvArSf72/1ZcArfcdOtdqx6pKkEVk8x+M/WlX7krwf2J3kL47TNgNqdZx69wS9ANoMcP7558+0r5KkIc3pyqGq9rWf+4E/offM4EftdhHt5/7WfApY0Xf4cmDfceqDPm9LVY1X1fjY2Nhcui5JOo5Zh0OSv5/kXUfWgcuBZ4EdwJEZRxuBB9r6DuC6NmtpLfB6u+20C7g8ydL2IPryVpMkjchcbit9APiTJEfO84dV9d+TPA7cl2QT8EPgU639TuAqYBJ4A/g0QFUdTPIl4PHW7otVdXAO/ZIkzdGsw6GqXgR+cUD9r4HLBtQLuP4Y59oKbJ1tXyRJ88tvSEuSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUsdJEw5J1iV5IclkkhtH3R9JOpOdFOGQZBFwB3AlsBq4Nsnq0fZKks5cJ0U4ABcDk1X1YlX9DNgOrB9xnyTpjHWyhMMy4JW+7alWkySNwOJRd6DJgFp1GiWbgc1t86dJXpjl550L/HiWx85abl3oT/z/jGTMI+aYT39n2njJrXMe8z8YptHJEg5TwIq+7eXAvqMbVdUWYMtcPyzJRFWNz/U8pxLHfGY408Z8po0XFm7MJ8ttpceBVUkuSHI2sAHYMeI+SdIZ66S4cqiqw0luAHYBi4CtVbVnxN2SpDPWSREOAFW1E9i5QB8351tTpyDHfGY408Z8po0XFmjMqeo895UkneFOlmcOkqSTyGkdDtO9kiPJOUnubfsfTbJy4Xs5f4YY779L8lySp5M8lGSoKW0ns2Ffu5LkmiSV5JSf2TLMmJP8Svu13pPkDxe6j/NtiN/b5yd5OMmT7ff3VaPo53xJsjXJ/iTPHmN/ktze/ns8neSiee9EVZ2WC70H238J/EPgbOAHwOqj2vxr4Pfa+gbg3lH3+wSP95eAv9fWP3Mqj3fYMbd27wK+CzwCjI+63wvw67wKeBJY2rbfP+p+L8CYtwCfaeurgZdH3e85jvmfAhcBzx5j/1XAt+h9R2wt8Oh89+F0vnIY5pUc64Ftbf1+4LIkg76QdyqYdrxV9XBVvdE2H6H3fZJT2bCvXfkS8B+Bv13Izp0gw4z5XwJ3VNUhgKrav8B9nG/DjLmAd7f19zDge1Knkqr6LnDwOE3WA/dUzyPAkiTnzWcfTudwGOaVHH/XpqoOA68D71uQ3s2/mb6CZBO9v3mcyqYdc5IPAyuq6sGF7NgJNMyv8y8Av5DkfyV5JMm6BevdiTHMmL8A/GqSKXqzHv/NwnRtZE74K4dOmqmsJ8Awr+QY6rUdp4ihx5LkV4Fx4J+d0B6deMcdc5K3AbcBv75QHVoAw/w6L6Z3a+lj9K4O/2eSC6vqtRPctxNlmDFfC9xdVV9N8k+Ab7Qx/98T372ROOF/dp3OVw7DvJLj79okWUzvcvR4l3Ins6FeQZLk48B/AD5ZVW8uUN9OlOnG/C7gQuA7SV6md292xyn+UHrY39cPVNX/qaqXgBfohcWpapgxbwLuA6iq7wNvp/fepdPVUP+/z8XpHA7DvJJjB7CxrV8DfLva055T0LTjbbdY/gu9YDjV70PDNGOuqter6tyqWllVK+k9Z/lkVU2MprvzYpjf139Kb/IBSc6ld5vpxQXt5fwaZsw/BC4DSPJBeuFwYEF7ubB2ANe1WUtrgder6tX5/IDT9rZSHeOVHEm+CExU1Q7gLnqXn5P0rhg2jK7HczPkeP8T8E7gv7Xn7j+sqk+OrNNzNOSYTytDjnkXcHmS54C3gN+qqr8eXa/nZsgxfw74epJ/S+/2yq+fwn/RI8k36d0WPLc9R7kZOAugqn6P3nOVq4BJ4A3g0/Peh1P4v58k6QQ5nW8rSZJmyXCQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkd/w8rnSQUT9U9/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2143d860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(yhat[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.mean(yhat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23756, 0.35966, 0.52944, 0.1272 , 0.1222 , 0.19194, 0.639  ,\n",
       "       0.45776, 0.68214, 0.33076])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7922815666376778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "print(roc_auc_score(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian treatment of the parameters in logistic regression naturally is not the end, but rather the beginning of my work with Bayesian models. Nevertheless, I've got some homework to do in statistics, tensorflow and edward!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [edward]",
   "language": "python",
   "name": "Python [edward]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
