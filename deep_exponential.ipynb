{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/blei-lab/edward/blob/master/examples/deep_exponential_family.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from edward.models import Gamma, Poisson, Normal, PointMass, \\\n",
    "    TransformedDistribution\n",
    "from edward.util import Progbar\n",
    "from observations import nips\n",
    "\n",
    "tf.flags.DEFINE_string(\"data_dir\", default=\"~/data\", help=\"\")\n",
    "tf.flags.DEFINE_string(\"logdir\", default=\"~/log/def/\", help=\"\")\n",
    "tf.flags.DEFINE_list(\"K\", default=[100, 30, 15],\n",
    "                     help=\"Number of components per layer.\")\n",
    "tf.flags.DEFINE_string(\"q\", default=\"lognormal\",\n",
    "                       help=\"Choice of q; 'lognormal' or 'gamma'.\")\n",
    "tf.flags.DEFINE_float(\"shape\", default=0.1, help=\"Gamma shape parameter.\")\n",
    "tf.flags.DEFINE_float(\"lr\", default=1e-4, help=\"Learning rate step-size.\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.data_dir = os.path.expanduser(FLAGS.data_dir)\n",
    "FLAGS.logdir = os.path.expanduser(FLAGS.logdir)\n",
    "timestamp = datetime.strftime(datetime.utcnow(), \"%Y%m%d_%H%M%S\")\n",
    "FLAGS.logdir += timestamp + '_' + '_'.join([str(ks) for ks in FLAGS.K]) + \\\n",
    "    '_q_' + str(FLAGS.q) + '_lr_' + str(FLAGS.lr)\n",
    "\n",
    "\n",
    "def pointmass_q(shape, name=None):\n",
    "  with tf.variable_scope(name, default_name=\"pointmass_q\"):\n",
    "    min_mean = 1e-3\n",
    "    mean = tf.get_variable(\"mean\", shape)\n",
    "    rv = PointMass(tf.maximum(tf.nn.softplus(mean), min_mean))\n",
    "    return rv\n",
    "\n",
    "\n",
    "def gamma_q(shape, name=None):\n",
    "  # Parameterize Gamma q's via shape and scale, with softplus unconstraints.\n",
    "  with tf.variable_scope(name, default_name=\"gamma_q\"):\n",
    "    min_shape = 1e-3\n",
    "    min_scale = 1e-5\n",
    "    shape = tf.get_variable(\n",
    "        \"shape\", shape,\n",
    "        initializer=tf.random_normal_initializer(mean=0.5, stddev=0.1))\n",
    "    scale = tf.get_variable(\n",
    "        \"scale\", shape, initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    rv = Gamma(tf.maximum(tf.nn.softplus(shape), min_shape),\n",
    "               tf.maximum(1.0 / tf.nn.softplus(scale), 1.0 / min_scale))\n",
    "    return rv\n",
    "\n",
    "\n",
    "def lognormal_q(shape, name=None):\n",
    "  with tf.variable_scope(name, default_name=\"lognormal_q\"):\n",
    "    min_scale = 1e-5\n",
    "    loc = tf.get_variable(\"loc\", shape)\n",
    "    scale = tf.get_variable(\n",
    "        \"scale\", shape, initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    rv = TransformedDistribution(\n",
    "        distribution=Normal(loc, tf.maximum(tf.nn.softplus(scale), min_scale)),\n",
    "        bijector=tf.contrib.distributions.bijectors.Exp())\n",
    "    return rv\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  ed.set_seed(42)\n",
    "\n",
    "  # DATA\n",
    "  x_train, metadata = nips(FLAGS.data_dir)\n",
    "  documents = metadata['columns']\n",
    "  words = metadata['rows']\n",
    "\n",
    "  # Subset to documents in 2011 and words appearing in at least two\n",
    "  # documents and have a total word count of at least 10.\n",
    "  doc_idx = [i for i, document in enumerate(documents)\n",
    "             if document.startswith('2011')]\n",
    "  documents = [documents[doc] for doc in doc_idx]\n",
    "  x_train = x_train[:, doc_idx]\n",
    "  word_idx = np.logical_and(np.sum(x_train != 0, 1) >= 2,\n",
    "                            np.sum(x_train, 1) >= 10)\n",
    "  words = [word for word, idx in zip(words, word_idx) if idx]\n",
    "  x_train = x_train[word_idx, :]\n",
    "  x_train = x_train.T\n",
    "\n",
    "  N = x_train.shape[0]  # number of documents\n",
    "  D = x_train.shape[1]  # vocabulary size\n",
    "\n",
    "  # MODEL\n",
    "  W2 = Gamma(0.1, 0.3, sample_shape=[FLAGS.K[2], FLAGS.K[1]])\n",
    "  W1 = Gamma(0.1, 0.3, sample_shape=[FLAGS.K[1], FLAGS.K[0]])\n",
    "  W0 = Gamma(0.1, 0.3, sample_shape=[FLAGS.K[0], D])\n",
    "\n",
    "  z3 = Gamma(0.1, 0.1, sample_shape=[N, FLAGS.K[2]])\n",
    "  z2 = Gamma(FLAGS.shape, FLAGS.shape / tf.matmul(z3, W2))\n",
    "  z1 = Gamma(FLAGS.shape, FLAGS.shape / tf.matmul(z2, W1))\n",
    "  x = Poisson(tf.matmul(z1, W0))\n",
    "\n",
    "  # INFERENCE\n",
    "  qW2 = pointmass_q(W2.shape)\n",
    "  qW1 = pointmass_q(W1.shape)\n",
    "  qW0 = pointmass_q(W0.shape)\n",
    "  if FLAGS.q == 'gamma':\n",
    "    qz3 = gamma_q(z3.shape)\n",
    "    qz2 = gamma_q(z2.shape)\n",
    "    qz1 = gamma_q(z1.shape)\n",
    "  else:\n",
    "    qz3 = lognormal_q(z3.shape)\n",
    "    qz2 = lognormal_q(z2.shape)\n",
    "    qz1 = lognormal_q(z1.shape)\n",
    "\n",
    "  # We apply variational EM with E-step over local variables\n",
    "  # and M-step to point estimate the global weight matrices.\n",
    "  inference_e = ed.KLqp({z1: qz1, z2: qz2, z3: qz3},\n",
    "                        data={x: x_train, W0: qW0, W1: qW1, W2: qW2})\n",
    "  inference_m = ed.MAP({W0: qW0, W1: qW1, W2: qW2},\n",
    "                       data={x: x_train, z1: qz1, z2: qz2, z3: qz3})\n",
    "\n",
    "  optimizer_e = tf.train.RMSPropOptimizer(FLAGS.lr)\n",
    "  optimizer_m = tf.train.RMSPropOptimizer(FLAGS.lr)\n",
    "  kwargs = {'optimizer': optimizer_e,\n",
    "            'n_print': 100,\n",
    "            'logdir': FLAGS.logdir,\n",
    "            'log_timestamp': False}\n",
    "  if FLAGS.q == 'gamma':\n",
    "    kwargs['n_samples'] = 30\n",
    "  inference_e.initialize(**kwargs)\n",
    "  inference_m.initialize(optimizer=optimizer_m)\n",
    "\n",
    "  sess = ed.get_session()\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  n_epoch = 20\n",
    "  n_iter_per_epoch = 10000\n",
    "  for epoch in range(n_epoch):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    nll = 0.0\n",
    "\n",
    "    pbar = Progbar(n_iter_per_epoch)\n",
    "    for t in range(1, n_iter_per_epoch + 1):\n",
    "      pbar.update(t)\n",
    "      info_dict_e = inference_e.update()\n",
    "      info_dict_m = inference_m.update()\n",
    "      nll += info_dict_e['loss']\n",
    "\n",
    "    # Compute perplexity averaged over a number of training iterations.\n",
    "    # The model's negative log-likelihood of data is upper bounded by\n",
    "    # the variational objective.\n",
    "    nll /= n_iter_per_epoch\n",
    "    perplexity = np.exp(nll / np.sum(x_train))\n",
    "    print(\"Negative log-likelihood <= {:0.3f}\".format(nll))\n",
    "    print(\"Perplexity <= {:0.3f}\".format(perplexity))\n",
    "\n",
    "    # Print top 10 words for first 10 topics.\n",
    "    qW0_vals = sess.run(qW0)\n",
    "    for k in range(10):\n",
    "      top_words_idx = qW0_vals[k, :].argsort()[-10:][::-1]\n",
    "      top_words = \" \".join([words[i] for i in top_words_idx])\n",
    "      print(\"Topic {}: {}\".format(k, top_words))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [edward]",
   "language": "python",
   "name": "Python [edward]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
